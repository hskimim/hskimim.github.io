---
title: "최대 가능도 모수 추정 (Maximum Likelihood Estimation)"
date: 2019-01-03 08:26:28 -0400
categories: [Machine_Learning]
tags: [Machine_Learning,HS]
---

본 포스팅은 카이스트 문일철 교수님의 [강좌](https://www.edwith.org/machinelearning1_17/joinLectures/9738)를 바탕으로 만들어졌습니다. 이미지 자료는 모두 문일철 교수님의 강좌 노트에서 첨부한 것입니다. 문제 시 바로 내리겠습니다.

```
본 포스팅은 단순히 개인 공부 정리 용도이며, 저자는 배경 지식이 많이 부족한 이유로, 최선을 다하겠지만 분명히 완벽하지 않거나, 틀린 부분이 있을 수 있습니다. 계속해서 수정해나가도록 하겠습니다.
```

- 사건을 예시로 들어볼까요??
    - 압정(thumbstack) 을 던진다고 생각해봅시다!! 아무래도 동전보다는 덜 fair 하겠죠?!
    - 총 다섯번 던져서 앞면이 3번 뒷 면이 2번이 나왔습니다.

- 우선 압정을 5번 던지는 시행에 대해 어떤 분포로 나타낼 수 있는 지에 대해 생각해볼까요?!

### Binomial Distribution
- 이산 확률 분포(discrete probability distribution)
    - 한 번의 시행에는 2 가지 경우만 존재할 수 있습니다. 이벤트의 결과가 1 또는 0 이라면 1이 나올 확률을 θ 라고 하고, 0이 나올 확률은 1 - θ 이 됩니다.
- Bernoulli distribution 과의 차이점은 다음과 같습니다.
    - Bernoulli : 동전을 한 번 던진다!
    - Binomial : 동전을 여러 번 던진다!
- Bernoulli , Binomial 모두 사건(K)가 2 인 것을 알 수 있습니다.

#### Binomial Distribution 의 수학적 description 을 알아봅시다!!

 <img src = "/images/post_img/binomial distribution.png">
 - f(k;n,p) 의 수식적 표현을 세분화시켜 보면,
    - 모수가 p일 때, n 번을 던졌을 때, k 번이 postive 가 나올 확률을 의미합니다.

- 동전을 5번 던졌는데, 2번이 나올 확률은?
    - 동전이 fair 하다는 가정 아래에서, `f(2;5,0.5)` 가 되겠습니다.


## Maximum Likelihood Estimation
- 압정 던지기 시행에 대한 확률 분포를 binomial distribution이라고 가정하였습니다.
- 즉, 우리의 hyphothesis 는,
    - `압정 던지기에 대한 확률분포는 모수 θ를 가지는 이항 분포일꺼야!` 가 됩니다.
- 우리의 가설을 강건하게 만들 수 있는 방법이 무엇이 있을가요?
    1. 더 적합한 분포를 찾는다.
    2. 가장 적합한 모수 θ 를 찾는다. -- how?

- MLE는 우리가 내린 확률 분포에 대한 가설을 강하게 만들어주는 모수를 추정하는 방법 중 하나입니다.
- MLE는 Maximum Likelihood Estimation 이라는 말 그대로, 어떤 모수 θ 가 주어졌을 때, 해당 분포의 확률이 가장 잘 나오는 θ를 찾는 것입니다.

<img src = "/images/post_img/mle.png">

- P(D|θ) 를 해석해보면 "어떤 모수 θ가 주어졌을 때, 우리가 정한 확률 분포의 probability" 입니다. 즉, 우리가 가설 내에서 어떠한 확률 분포를 가정했을 때, 어떠한 모수가 주어져야 가장 P 가 커질지에 대한 문제입니다.

## Maximum Likelihood Estimation

<img src = "/images/post_img/mle.png">
- 수식을 다시 한 번 살펴보면 최적화(maximizing) 문제임을 알 수 있습니다. 그리고, 최적화 문제에는 미분(derivative) 를 적용하면 되겠죠?!
- MLE에서는 최적화 과정에서 대소 관계에 영향을 끼치지 않으면서, 미분 과정에서 연산을 쉽게 해주기 위해서 log transform 을 진행합니다. 즉, log Likelihood 를 취해주는 것이죠.

1 θ 의 추정치, θ_hat은 binomial distribution을 따릅니다.

<img src = "/images/post_img/binomial_dist_theta_hat.png">

2 log transform 을 통해서, 곱셈 연산이 덧셉 연산으로 바뀌게 되고, 멱함수의 승수가 앞의 곱셈으로 변환되었으며, 곱셈이 뎃셈 연산으로 변환된 것을 볼 수 있습니다.

<img src = "/images/post_img/log_transform_in_mle.png">

3 argmax_θ에 따라서, 최적화를 시켜주는 parameter θ 를 구하는 문제입니다. 이에 따라서 θ로 편미분을 취해주게 됩니다. 추가적인 연산을 진행하여 최종 θ 를 구하게 됩니다.

<img src = "/images/post_img/final_mle.png">

우리가 최종적으로 계산한 θ 에는 사실 상, 우리가 늘 해오던 직관적 방식이 녹아져 있습니다. 5번을 던져 2번이 앞면이 나오게 되면, MLE에 따른 최적의 모수는 2/5(0.4) 가 되는 것입니다.

## Simple Error Bound
- 여태까지 MLE를 통해서 최적의 파라미터 θ를 계산했습니다. 그렇다면, 압정 던지기에 따른 모수를 구해낸 것일까요?! 압정을 던질 때마다 계속 다른 값들이 나올 것입니다.(i.i.d 에 따라서) 이에 따라, 우리가 힘들게 구한 모수는 계속해서 변하게 될 것입니다. 그렇다면 어떻게 안정적인 모수를 계산할 수 있을까요?!
<img src = "/images/post_img/simple error bound.png">
수학 필드에서 사용되는 공식이지만, 위 공식은 샘플링 과정에 따른 에러(error)의 범위를 bounding해주는 부등식입니다. N이 시행 횟수로, 그에 따라 N이 커지면 등식의 오른쪽 요소가 작아지게 되고 그에 따라 에러의 텀이 작아지게 되는 것이죠.
- 즉, 많이 관측할 수록 모수의 에러가 작아진다가 됩니다.

____________

다음 포스팅에서는 베이지안 정리를 기반으로 한 MAP(Maximum a Posteriori Estimation)에 대해서 다루어 보도록 하겠습니다.
